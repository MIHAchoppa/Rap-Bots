 sun_musev.mp4 
 output.mp4 
 9.mp4 
 11.mp4 
 sli_480P.mp4 
 talk_480P.mp4 
MuseTalk 1.0
 8_8.mp4 
 output_V1.mp4 
 9_9.mp4 
 11_11.mp4 
 silent_v1.mp4 
 talk_v1.mp4 
MuseTalk 1.5
 8_8.mp4 
 V1.5.mp4 
 9_9.mp4 
 11_11.mp4 
 silent_v15.mp4 
 talk_v15.mp4 
TODO:
 trained models and inference codes.
 Huggingface Gradio demo.
 codes for real-time inference.
 technical report.
 a better model with updated technical report.
 realtime inference code for 1.5 version.
 training and data preprocessing codes.
 always welcome to submit issues and PRs to improve this repository! ðŸ˜Š
Getting Started
We provide a detailed tutorial about the installation and the basic usage of MuseTalk for new users:

Third party integration
Thanks for the third-party integration, which makes installation and use more convenient for everyone. We also hope you note that we have not verified, maintained, or updated third-party. Please refer to this project for specific results.

ComfyUI
Installation
To prepare the Python environment and install additional packages such as opencv, diffusers, mmcv, etc., please follow the steps below:

Build environment
We recommend Python 3.10 and CUDA 11.7. Set up your environment as follows:

conda create -n MuseTalk python==3.10
conda activate MuseTalk
Install PyTorch 2.0.1
Choose one of the following installation methods:

# Option 1: Using pip
pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118

# Option 2: Using conda
conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia
Install Dependencies
Install the remaining required packages:

pip install -r requirements.txt
Install MMLab Packages
Install the MMLab ecosystem packages:

pip install --no-cache-dir -U openmim
mim install mmengine
mim install "mmcv==2.0.1"
mim install "mmdet==3.1.0"
mim install "mmpose==1.1.0"
Setup FFmpeg
Download the ffmpeg-static package

Configure FFmpeg based on your operating system:

For Linux:

export FFMPEG_PATH=/path/to/ffmpeg
# Example:
export FFMPEG_PATH=/musetalk/ffmpeg-4.4-amd64-static
For Windows: Add the ffmpeg-xxx\bin directory to your system's PATH environment variable. Verify the installation by running ffmpeg -version in the command prompt - it should display the ffmpeg version information.

Download weights
You can download weights in two ways:

Option 1: Using Download Scripts
We provide two scripts for automatic downloading:

For Linux:

sh ./download_weights.sh
For Windows:

# Run the script
download_weights.bat
Option 2: Manual Download
You can also download the weights manually from the following links:

Download our trained weights
Download the weights of other components:
sd-vae-ft-mse
whisper
dwpose
syncnet
face-parse-bisent
resnet18
Finally, these weights should be organized in models as follows:

./models/
â”œâ”€â”€ musetalk
â”‚   â””â”€â”€ musetalk.json
â”‚   â””â”€â”€ pytorch_model.bin
â”œâ”€â”€ musetalkV15
â”‚   â””â”€â”€ musetalk.json
â”‚   â””â”€â”€ unet.pth
â”œâ”€â”€ syncnet
â”‚   â””â”€â”€ latentsync_syncnet.pt
â”œâ”€â”€ dwpose
â”‚   â””â”€â”€ dw-ll_ucoco_384.pth
â”œâ”€â”€ face-parse-bisent
â”‚   â”œâ”€â”€ 79999_iter.pth
â”‚   â””â”€â”€ resnet18-5c106cde.pth
â”œâ”€â”€ sd-vae
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ diffusion_pytorch_model.bin
â””â”€â”€ whisper
    â”œâ”€â”€ config.json
    â”œâ”€â”€ pytorch_model.bin
    â””â”€â”€ preprocessor_config.json
    
Quickstart
Inference
We provide inference scripts for both versions of MuseTalk:

Prerequisites
Before running inference, please ensure ffmpeg is installed and accessible:

# Check ffmpeg installation
ffmpeg -version
If ffmpeg is not found, please install it first:

Windows: Download from ffmpeg-static and add to PATH
Linux: sudo apt-get install ffmpeg
Normal Inference
Linux Environment
# MuseTalk 1.5 (Recommended)
sh inference.sh v1.5 normal

# MuseTalk 1.0
sh inference.sh v1.0 normal
Windows Environment
Please ensure that you set the ffmpeg_path to match the actual location of your FFmpeg installation.

# MuseTalk 1.5 (Recommended)
python -m scripts.inference --inference_config configs\inference\test.yaml --result_dir results\test --unet_model_path models\musetalkV15\unet.pth --unet_config models\musetalkV15\musetalk.json --version v15 --ffmpeg_path ffmpeg-master-latest-win64-gpl-shared\bin

# For MuseTalk 1.0, change:
# - models\musetalkV15 -> models\musetalk
# - unet.pth -> pytorch_model.bin
# - --version v15 -> --version v1
Real-time Inference
Linux Environment
# MuseTalk 1.5 (Recommended)
sh inference.sh v1.5 realtime

# MuseTalk 1.0
sh inference.sh v1.0 realtime
Windows Environment
# MuseTalk 1.5 (Recommended)
python -m scripts.realtime_inference --inference_config configs\inference\realtime.yaml --result_dir results\realtime --unet_model_path models\musetalkV15\unet.pth --unet_config models\musetalkV15\musetalk.json --version v15 --fps 25 --ffmpeg_path ffmpeg-master-latest-win64-gpl-shared\bin

# For MuseTalk 1.0, change:
# - models\musetalkV15 -> models\musetalk
# - unet.pth -> pytorch_model.bin
# - --version v15 -> --version v1
The configuration file configs/inference/test.yaml contains the inference settings, including:

video_path: Path to the input video, image file, or directory of images
audio_path: Path to the input audio file
Note: For optimal results, we recommend using input videos with 25fps, which is the same fps used during model training. If your video has a lower frame rate, you can use frame interpolation or convert it to 25fps using ffmpeg.

Important notes for real-time inference:

Set preparation to True when processing a new avatar
After preparation, the avatar will generate videos using audio clips from audio_clips
The generation process can achieve 30fps+ on an NVIDIA Tesla V100
Set preparation to False for generating more videos with the same avatar
For faster generation without saving images, you can use:

python -m scripts.realtime_inference --inference_config configs/inference/realtime.yaml --skip_save_images
Gradio Demo
We provide an intuitive web interface through Gradio for users to easily adjust input parameters. To optimize inference time, users can generate only the first frame to fine-tune the best lip-sync parameters, which helps reduce facial artifacts in the final output. para For minimum hardware requirements, we tested the system on a Windows environment using an NVIDIA GeForce RTX 3050 Ti Laptop GPU with 4GB VRAM. In fp16 mode, generating an 8-second video takes approximately 5 minutes. speed

Both Linux and Windows users can launch the demo using the following command. Please ensure that the ffmpeg_path parameter matches your actual FFmpeg installation path:

# You can remove --use_float16 for better quality, but it will increase VRAM usage and inference time
python app.py --use_float16 --ffmpeg_path ffmpeg-master-latest-win64-gpl-shared\bin
Training
Data Preparation
To train MuseTalk, you need to prepare your dataset following these steps:

Place your source videos

For example, if you're using the HDTF dataset, place all your video files in ./dataset/HDTF/source.

Run the preprocessing script

python -m scripts.preprocess --config ./configs/training/preprocess.yaml
This script will:

Extract frames from videos
Detect and align faces
Generate audio features
Create the necessary data structure for training
Training Process
After data preprocessing, you can start the training process:

First Stage

sh train.sh stage1
Second Stage

sh train.sh stage2
Configuration Adjustment
Before starting the training, you should adjust the configuration files according to your hardware and requirements:

GPU Configuration (configs/training/gpu.yaml):

gpu_ids: Specify the GPU IDs you want to use (e.g., "0,1,2,3")
num_processes: Set this to match the number of GPUs you're using
Stage 1 Configuration (configs/training/stage1.yaml):

data.train_bs: Adjust batch size based on your GPU memory (default: 32)
data.n_sample_frames: Number of sampled frames per video (default: 1)
Stage 2 Configuration (configs/training/stage2.yaml):

random_init_unet: Must be set to False to use the model from stage 1
data.train_bs: Smaller batch size due to high GPU memory cost (default: 2)
data.n_sample_frames: Higher value for temporal consistency (default: 16)
solver.gradient_accumulation_steps: Increase to simulate larger batch sizes (default: 8)
GPU Memory Requirements
Based on our testing on a machine with 8 NVIDIA H20 GPUs:

Stage 1 Memory Usage
Batch Size	Gradient Accumulation	Memory per GPU	Recommendation
8	1	~32GB	
16	1	~45GB	
32	1	~74GB	âœ“
Stage 2 Memory Usage
Batch Size	Gradient Accumulation	Memory per GPU	Recommendation
1	8	~54GB	
2	2	~80GB	
2	8	~85GB	âœ“
Details
## TestCases For 1.0
Image	MuseV	+MuseTalk
	
 musk_musev.mp4 
 musk_musetalk.mp4 
	
 yongen_musev.mp4 
 yongen_musetalk.mp4 
	
 sit_musev.mp4 
 sit_musetalkmp4.mp4 
	
 man_musev.mp4 
 man_musetalk.mp4 
	
 monalisa_musev.mp4 
 monalisa_musetalk.mp4 
	
 sun_musev.mp4 
 sun_musetalk.mp4 
	
 sun_musev.mp4 
 sun_musetalk.mp4 
Use of bbox_shift to have adjustable results(For 1.0)
ðŸ”Ž We have found that upper-bound of the mask has an important impact on mouth openness. Thus, to control the mask region, we suggest using the bbox_shift parameter. Positive values (moving towards the lower half) increase mouth openness, while negative values (moving towards the upper half) decrease mouth openness.

You can start by running with the default configuration to obtain the adjustable value range, and then re-run the script within this range.

For example, in the case of Xinying Sun, after running the default configuration, it shows that the adjustable value rage is [-9, 9]. Then, to decrease the mouth openness, we set the value to be -7.

python -m scripts.inference --inference_config configs/inference/test.yaml --bbox_shift -7 
ðŸ“Œ More technical details can be found in bbox_shift.

Combining MuseV and MuseTalk
As a complete solution to virtual human generation, you are suggested to first apply MuseV to generate a video (text-to-video, image-to-video or pose-to-video) by referring this. Frame interpolation is suggested to increase frame rate. Then, you can use MuseTalk to generate a lip-sync video by referring this.

Acknowledgement
We thank open-source components like whisper, dwpose, face-alignment, face-parsing, S3FD and LatentSync.
MuseTalk has referred much to diffusers and isaacOnline/whisper.
MuseTalk has been built on HDTF datasets.
Thanks for open-sourcing!

Limitations
Resolution: Though MuseTalk uses a face region size of 256 x 256, which make it better than other open-source methods, it has not yet reached the theoretical resolution bound. We will continue to deal with this problem.
If you need higher resolution, you could apply super resolution models such as GFPGAN in combination with MuseTalk.

Identity preservation: Some details of the original face are not well preserved, such as mustache, lip shape and color.

Jitter: There exists some jitter as the current pipeline adopts single-frame generation.

Citation
@article{musetalk,
  title={MuseTalk: Real-Time High-Fidelity Video Dubbing via Spatio-Temporal Sampling},
  author={Zhang, Yue and Zhong, Zhizhou and Liu, Minhao and Chen, Zhaokang and Wu, Bin and Zeng, Yubin and Zhan, Chao and He, Yingjie and Huang, Junxin and Zhou, Wenjiang},
  journal={arxiv},
  year={2025}
}
Disclaimer/License
code: The code of MuseTalk is released under the MIT License. There is no limitation for both academic and commercial usage.
model: The trained model are available for any purpose, even commercially.
other opensource model: Other open-source models used must comply with their license, such as whisper, ft-mse-vae, dwpose, S3FD, etc..
The testdata are collected from internet, which are available for non-commercial research purposes only.
AIGC: This project strives to impact the domain of AI-driven video generation positively. Users are granted the freedom to create videos using this tool, but they are expected to comply with local laws and utilize it responsibly. The developers do not assume any responsibility for potential misuse by users.